{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from    tensorflow.keras import datasets, layers, optimizers, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: (60000, 28, 28) (60000,) 0 255\n"
     ]
    }
   ],
   "source": [
    "(x, y), (x_test, y_test) = datasets.mnist.load_data()#下载或读取数据集\n",
    "print('datasets:', x.shape, y.shape, x.min(), x.max())#打印\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs = x_test[0:5]#选取第0到5的图片\n",
    "labs = y_test[0:5]\n",
    "print(labs)\n",
    "plot_imgs = np.hstack(imgs)#将五张图片拼接成一行\n",
    "plt.imshow(plot_imgs, cmap='gray')#选择gray灰度图\n",
    "pylab.show()#显示测试图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val=tf.split(x,num_or_size_splits=[55000,5000])\n",
    "y_train,y_val=tf.split(y,num_or_size_splits=[55000,5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 100#设置批量大小\n",
    "db = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "db = db.map(preprocess).shuffle(55000).batch(batchsz).repeat(10)\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "db_test = db_test.map(preprocess).batch(batchsz)\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "ds_val = ds_val.map(preprocess).batch(batchsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (100, 26, 26, 6)          60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (100, 13, 13, 6)          0         \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (100, 13, 13, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (100, 11, 11, 16)         880       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (100, 5, 5, 16)           0         \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (100, 5, 5, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (100, 400)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (100, 120)                48120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, 60)                 7260      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (100, 10)                 610       \n",
      "=================================================================\n",
      "Total params: 56,930\n",
      "Trainable params: 56,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#构建网络模型\n",
    "network = Sequential([\n",
    "    \n",
    "    layers.Conv2D(6,kernel_size=3,strides=1),\n",
    "    layers.MaxPooling2D(pool_size=2,strides=2),\n",
    "    layers.ReLU(),\n",
    "    \n",
    "    layers.Conv2D(16,kernel_size=3,strides=1),\n",
    "    layers.MaxPooling2D(pool_size=2,strides=2),\n",
    "    layers.ReLU(),\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(120,activation='relu'),\n",
    "    layers.Dense(60,activation='relu'),\n",
    "    \n",
    "    \n",
    "    \n",
    "                     layers.Dense(10,)]\n",
    "                    )\n",
    "network.build(input_shape=(batchsz, 28,28,1))\n",
    "network.summary()#打印网络参数\n",
    "# 设置优化器\n",
    "mydecay = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "initial_learning_rate=0.01,\n",
    "decay_steps=1000,\n",
    "decay_rate=0.96)\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = optimizers.Adam(mydecay)# 固定学习率的Adam学习算法\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.01)# 固定学习率的Adam学习算法\n",
    "#optimizer  =  optimizers.SGD(learning_rate=0.01)# 固定学习率的SGD学习算法\n",
    "#optimizer = optimizers.Adagrad(learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo= 0  step= 0 loss: 2.146089553833008\n",
      " step= 0 Evaluate Acc: 0.3152\n",
      "echo= 0  step= 100 loss: 0.13026075065135956\n",
      "echo= 0  step= 200 loss: 0.058109816163778305\n",
      "echo= 0  step= 300 loss: 0.02898886613547802\n",
      "echo= 0  step= 400 loss: 0.02152613177895546\n",
      "echo= 0  step= 500 loss: 0.041263796389102936\n",
      " step= 500 Evaluate Acc: 0.978\n",
      "echo= 0  step= 600 loss: 0.0945831686258316\n",
      "echo= 0  step= 700 loss: 0.10042037814855576\n",
      "echo= 0  step= 800 loss: 0.058618441224098206\n",
      "echo= 0  step= 900 loss: 0.038483213633298874\n",
      "echo= 0  step= 1000 loss: 0.08834123611450195\n",
      " step= 1000 Evaluate Acc: 0.9872\n",
      "echo= 0  step= 1100 loss: 0.036627963185310364\n",
      "echo= 0  step= 1200 loss: 0.008271502330899239\n",
      "echo= 0  step= 1300 loss: 0.028947968035936356\n",
      "echo= 0  step= 1400 loss: 0.015237918123602867\n",
      "echo= 0  step= 1500 loss: 0.014919170178472996\n",
      " step= 1500 Evaluate Acc: 0.9854\n",
      "echo= 0  step= 1600 loss: 0.029730206355452538\n",
      "echo= 0  step= 1700 loss: 0.03895603492856026\n",
      "echo= 0  step= 1800 loss: 0.008290660567581654\n",
      "echo= 0  step= 1900 loss: 0.01007625088095665\n",
      "echo= 0  step= 2000 loss: 0.05242941901087761\n",
      " step= 2000 Evaluate Acc: 0.9858\n",
      "echo= 0  step= 2100 loss: 0.1506665050983429\n",
      "echo= 0  step= 2200 loss: 0.02648489736020565\n",
      "echo= 0  step= 2300 loss: 0.04733511805534363\n",
      "echo= 0  step= 2400 loss: 0.13107135891914368\n",
      "echo= 0  step= 2500 loss: 0.006245768163353205\n",
      " step= 2500 Evaluate Acc: 0.9848\n",
      "echo= 0  step= 2600 loss: 0.04043054208159447\n",
      "echo= 0  step= 2700 loss: 0.00914123933762312\n",
      "echo= 0  step= 2800 loss: 0.03701958805322647\n",
      "echo= 0  step= 2900 loss: 0.02193274348974228\n",
      "echo= 0  step= 3000 loss: 0.0029434978496283293\n",
      " step= 3000 Evaluate Acc: 0.9856\n",
      "echo= 0  step= 3100 loss: 0.03243911266326904\n",
      "echo= 0  step= 3200 loss: 0.05664949491620064\n",
      "echo= 0  step= 3300 loss: 0.06409533321857452\n",
      "echo= 0  step= 3400 loss: 0.11353124678134918\n",
      "echo= 0  step= 3500 loss: 0.00251202960498631\n",
      " step= 3500 Evaluate Acc: 0.9872\n",
      "echo= 0  step= 3600 loss: 0.1261744648218155\n",
      "echo= 0  step= 3700 loss: 0.020052844658493996\n",
      "echo= 0  step= 3800 loss: 0.013430667109787464\n",
      "echo= 0  step= 3900 loss: 0.06508617103099823\n",
      "echo= 0  step= 4000 loss: 0.07011613994836807\n",
      " step= 4000 Evaluate Acc: 0.9844\n",
      "echo= 0  step= 4100 loss: 0.002893978264182806\n",
      "echo= 0  step= 4200 loss: 0.057784851640462875\n",
      "echo= 0  step= 4300 loss: 0.0014876851346343756\n",
      "echo= 0  step= 4400 loss: 0.012097151018679142\n",
      "echo= 0  step= 4500 loss: 0.000827328534796834\n",
      " step= 4500 Evaluate Acc: 0.9888\n",
      "echo= 0  step= 4600 loss: 0.13410302996635437\n",
      "echo= 0  step= 4700 loss: 0.07412880659103394\n",
      "echo= 0  step= 4800 loss: 0.05253558233380318\n",
      "echo= 0  step= 4900 loss: 0.06667409837245941\n",
      "echo= 0  step= 5000 loss: 0.004626067355275154\n",
      " step= 5000 Evaluate Acc: 0.987\n",
      "echo= 0  step= 5100 loss: 0.09041395038366318\n",
      "echo= 0  step= 5200 loss: 0.005725853610783815\n",
      "echo= 0  step= 5300 loss: 0.014385931193828583\n",
      "echo= 0  step= 5400 loss: 0.017002196982502937\n",
      "echo= 1  step= 0 loss: 0.045630063861608505\n",
      " step= 0 Evaluate Acc: 0.9854\n",
      "echo= 1  step= 100 loss: 0.002837830688804388\n",
      "echo= 1  step= 200 loss: 0.08619809150695801\n",
      "echo= 1  step= 300 loss: 0.02201477251946926\n",
      "echo= 1  step= 400 loss: 0.05093429610133171\n",
      "echo= 1  step= 500 loss: 0.00474538654088974\n",
      " step= 500 Evaluate Acc: 0.984\n",
      "echo= 1  step= 600 loss: 0.00039975211257115006\n",
      "echo= 1  step= 700 loss: 0.004794385749846697\n",
      "echo= 1  step= 800 loss: 0.07121061533689499\n",
      "echo= 1  step= 900 loss: 0.03923892229795456\n",
      "echo= 1  step= 1000 loss: 0.000843607762362808\n",
      " step= 1000 Evaluate Acc: 0.9866\n",
      "echo= 1  step= 1100 loss: 0.035638805478811264\n",
      "echo= 1  step= 1200 loss: 0.039967939257621765\n",
      "echo= 1  step= 1300 loss: 0.07009883970022202\n",
      "echo= 1  step= 1400 loss: 0.0002078577526845038\n",
      "echo= 1  step= 1500 loss: 0.006001131609082222\n",
      " step= 1500 Evaluate Acc: 0.9854\n",
      "echo= 1  step= 1600 loss: 0.0009153799037449062\n",
      "echo= 1  step= 1700 loss: 0.09976890683174133\n",
      "echo= 1  step= 1800 loss: 0.004259388893842697\n",
      "echo= 1  step= 1900 loss: 0.06831567734479904\n",
      "echo= 1  step= 2000 loss: 0.060485485941171646\n",
      " step= 2000 Evaluate Acc: 0.9876\n",
      "echo= 1  step= 2100 loss: 0.13334177434444427\n",
      "echo= 1  step= 2200 loss: 0.03927580267190933\n",
      "echo= 1  step= 2300 loss: 0.0008739730692468584\n",
      "echo= 1  step= 2400 loss: 0.02985612116754055\n",
      "echo= 1  step= 2500 loss: 0.02039317600429058\n",
      " step= 2500 Evaluate Acc: 0.9838\n",
      "echo= 1  step= 2600 loss: 0.007942273281514645\n",
      "echo= 1  step= 2700 loss: 0.011915027163922787\n",
      "echo= 1  step= 2800 loss: 0.01359474379569292\n",
      "echo= 1  step= 2900 loss: 0.0014274615095928311\n",
      "echo= 1  step= 3000 loss: 0.010759725235402584\n",
      " step= 3000 Evaluate Acc: 0.9864\n",
      "echo= 1  step= 3100 loss: 0.006963747553527355\n",
      "echo= 1  step= 3200 loss: 0.08462134003639221\n",
      "echo= 1  step= 3300 loss: 0.008996749296784401\n",
      "echo= 1  step= 3400 loss: 0.010996498167514801\n",
      "echo= 1  step= 3500 loss: 0.001739446190185845\n",
      " step= 3500 Evaluate Acc: 0.9902\n",
      "echo= 1  step= 3600 loss: 0.028674064204096794\n",
      "echo= 1  step= 3700 loss: 0.0007877122843638062\n",
      "echo= 1  step= 3800 loss: 0.004864710383117199\n",
      "echo= 1  step= 3900 loss: 0.002334409160539508\n",
      "echo= 1  step= 4000 loss: 0.18726129829883575\n",
      " step= 4000 Evaluate Acc: 0.9896\n",
      "echo= 1  step= 4100 loss: 0.008086887188255787\n",
      "echo= 1  step= 4200 loss: 0.023859167471528053\n",
      "echo= 1  step= 4300 loss: 0.0016569143626838923\n",
      "echo= 1  step= 4400 loss: 0.08205650001764297\n",
      "echo= 1  step= 4500 loss: 0.01761733740568161\n",
      " step= 4500 Evaluate Acc: 0.983\n",
      "echo= 1  step= 4600 loss: 0.021090509369969368\n",
      "echo= 1  step= 4700 loss: 0.0009987620869651437\n",
      "echo= 1  step= 4800 loss: 0.002196410670876503\n",
      "echo= 1  step= 4900 loss: 0.0009018177515827119\n",
      "echo= 1  step= 5000 loss: 9.28311601455789e-06\n",
      " step= 5000 Evaluate Acc: 0.9896\n",
      "echo= 1  step= 5100 loss: 0.014559403993189335\n",
      "echo= 1  step= 5200 loss: 0.003973979968577623\n",
      "echo= 1  step= 5300 loss: 0.015126672573387623\n",
      "echo= 1  step= 5400 loss: 0.0019344210159033537\n",
      "echo= 2  step= 0 loss: 0.0014826938277110457\n",
      " step= 0 Evaluate Acc: 0.9884\n",
      "echo= 2  step= 100 loss: 0.040869615972042084\n",
      "echo= 2  step= 200 loss: 0.0021001347340643406\n",
      "echo= 2  step= 300 loss: 0.027183517813682556\n",
      "echo= 2  step= 400 loss: 0.00019107767730019987\n",
      "echo= 2  step= 500 loss: 0.0026601997669786215\n",
      " step= 500 Evaluate Acc: 0.984\n",
      "echo= 2  step= 600 loss: 0.009244904853403568\n",
      "echo= 2  step= 700 loss: 0.00028824564651586115\n",
      "echo= 2  step= 800 loss: 0.013229109346866608\n",
      "echo= 2  step= 900 loss: 0.0085531547665596\n",
      "echo= 2  step= 1000 loss: 0.000448347331257537\n",
      " step= 1000 Evaluate Acc: 0.9878\n",
      "echo= 2  step= 1100 loss: 9.568726818542928e-05\n",
      "echo= 2  step= 1200 loss: 0.00731627456843853\n",
      "echo= 2  step= 1300 loss: 0.0007552297902293503\n",
      "echo= 2  step= 1400 loss: 0.010790468193590641\n",
      "echo= 2  step= 1500 loss: 0.0023203310556709766\n",
      " step= 1500 Evaluate Acc: 0.9876\n",
      "echo= 2  step= 1600 loss: 0.00246597733348608\n",
      "echo= 2  step= 1700 loss: 0.012228387407958508\n",
      "echo= 2  step= 1800 loss: 0.005741478409618139\n",
      "echo= 2  step= 1900 loss: 0.06256533414125443\n",
      "echo= 2  step= 2000 loss: 0.42000144720077515\n",
      " step= 2000 Evaluate Acc: 0.9826\n",
      "echo= 2  step= 2100 loss: 0.00014713620475959033\n",
      "echo= 2  step= 2200 loss: 0.02572799287736416\n",
      "echo= 2  step= 2300 loss: 0.000475676148198545\n",
      "echo= 2  step= 2400 loss: 0.0034535317681729794\n",
      "echo= 2  step= 2500 loss: 0.0003169229603372514\n",
      " step= 2500 Evaluate Acc: 0.9874\n",
      "echo= 2  step= 2600 loss: 0.0008800191571936011\n",
      "echo= 2  step= 2700 loss: 0.057032510638237\n",
      "echo= 2  step= 2800 loss: 0.012353911064565182\n",
      "echo= 2  step= 2900 loss: 0.01421548705548048\n",
      "echo= 2  step= 3000 loss: 0.04005816578865051\n",
      " step= 3000 Evaluate Acc: 0.9884\n",
      "echo= 2  step= 3100 loss: 0.001296290778554976\n",
      "echo= 2  step= 3200 loss: 0.05228904262185097\n",
      "echo= 2  step= 3300 loss: 0.030039934441447258\n",
      "echo= 2  step= 3400 loss: 0.0014307470992207527\n",
      "echo= 2  step= 3500 loss: 0.07896889001131058\n",
      " step= 3500 Evaluate Acc: 0.9862\n",
      "echo= 2  step= 3600 loss: 0.09609948098659515\n",
      "echo= 2  step= 3700 loss: 0.08231081068515778\n",
      "echo= 2  step= 3800 loss: 0.0043822345323860645\n",
      "echo= 2  step= 3900 loss: 0.02119133435189724\n",
      "echo= 2  step= 4000 loss: 0.00011779950000345707\n",
      " step= 4000 Evaluate Acc: 0.9846\n",
      "echo= 2  step= 4100 loss: 0.0006041859742254019\n",
      "echo= 2  step= 4200 loss: 0.05953865498304367\n",
      "echo= 2  step= 4300 loss: 0.01853983663022518\n",
      "echo= 2  step= 4400 loss: 0.01904732920229435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo= 2  step= 4500 loss: 0.0015284568071365356\n",
      " step= 4500 Evaluate Acc: 0.9876\n",
      "echo= 2  step= 4600 loss: 0.0005415549967437983\n",
      "echo= 2  step= 4700 loss: 0.0009736301144585013\n",
      "echo= 2  step= 4800 loss: 0.1296556442975998\n",
      "echo= 2  step= 4900 loss: 0.019388919696211815\n",
      "echo= 2  step= 5000 loss: 0.00031691513140685856\n",
      " step= 5000 Evaluate Acc: 0.9876\n",
      "echo= 2  step= 5100 loss: 0.07954305410385132\n",
      "echo= 2  step= 5200 loss: 0.0035769843962043524\n",
      "echo= 2  step= 5300 loss: 0.041065678000450134\n",
      "echo= 2  step= 5400 loss: 0.031823817640542984\n",
      "echo= 3  step= 0 loss: 0.0010153712937608361\n",
      " step= 0 Evaluate Acc: 0.985\n",
      "echo= 3  step= 100 loss: 1.740473999234382e-05\n",
      "echo= 3  step= 200 loss: 0.029945308342576027\n",
      "echo= 3  step= 300 loss: 0.022233936935663223\n",
      "echo= 3  step= 400 loss: 0.005005809478461742\n",
      "echo= 3  step= 500 loss: 0.002953458111733198\n",
      " step= 500 Evaluate Acc: 0.9878\n",
      "echo= 3  step= 600 loss: 1.340791550319409e-05\n",
      "echo= 3  step= 700 loss: 0.0004901278298348188\n",
      "echo= 3  step= 800 loss: 2.520829366403632e-05\n",
      "echo= 3  step= 900 loss: 0.002926338231191039\n",
      "echo= 3  step= 1000 loss: 0.004992242902517319\n",
      " step= 1000 Evaluate Acc: 0.9852\n",
      "echo= 3  step= 1100 loss: 5.370597864384763e-05\n",
      "echo= 3  step= 1200 loss: 0.08497534692287445\n",
      "echo= 3  step= 1300 loss: 0.015604653395712376\n",
      "echo= 3  step= 1400 loss: 0.07161186635494232\n",
      "echo= 3  step= 1500 loss: 0.02806641347706318\n",
      " step= 1500 Evaluate Acc: 0.9898\n",
      "echo= 3  step= 1600 loss: 0.012214086949825287\n",
      "echo= 3  step= 1700 loss: 0.05191805958747864\n",
      "echo= 3  step= 1800 loss: 0.0038603441789746284\n",
      "echo= 3  step= 1900 loss: 0.09449077397584915\n",
      "echo= 3  step= 2000 loss: 9.500717510491086e-07\n",
      " step= 2000 Evaluate Acc: 0.9844\n",
      "echo= 3  step= 2100 loss: 0.13696250319480896\n",
      "echo= 3  step= 2200 loss: 0.0041398643516004086\n",
      "echo= 3  step= 2300 loss: 0.04706130176782608\n",
      "echo= 3  step= 2400 loss: 0.02767297998070717\n",
      "echo= 3  step= 2500 loss: 0.010786563158035278\n",
      " step= 2500 Evaluate Acc: 0.9878\n",
      "echo= 3  step= 2600 loss: 0.00039082535658963025\n",
      "echo= 3  step= 2700 loss: 9.869384666671976e-05\n",
      "echo= 3  step= 2800 loss: 0.013785324059426785\n",
      "echo= 3  step= 2900 loss: 0.0010451690759509802\n",
      "echo= 3  step= 3000 loss: 0.022755511105060577\n",
      " step= 3000 Evaluate Acc: 0.9868\n",
      "echo= 3  step= 3100 loss: 0.033732056617736816\n",
      "echo= 3  step= 3200 loss: 0.0028812841046601534\n",
      "echo= 3  step= 3300 loss: 8.561564754927531e-05\n",
      "echo= 3  step= 3400 loss: 0.00285289715975523\n",
      "echo= 3  step= 3500 loss: 3.775115328608081e-05\n",
      " step= 3500 Evaluate Acc: 0.9862\n",
      "echo= 3  step= 3600 loss: 7.45001234463416e-05\n",
      "echo= 3  step= 3700 loss: 0.12262871116399765\n",
      "echo= 3  step= 3800 loss: 0.24140077829360962\n",
      "echo= 3  step= 3900 loss: 0.0006211647996678948\n",
      "echo= 3  step= 4000 loss: 0.00020815413154195994\n",
      " step= 4000 Evaluate Acc: 0.9878\n",
      "echo= 3  step= 4100 loss: 0.20175579190254211\n",
      "echo= 3  step= 4200 loss: 0.08146156370639801\n",
      "echo= 3  step= 4300 loss: 0.032767154276371\n",
      "echo= 3  step= 4400 loss: 0.003684431314468384\n",
      "echo= 3  step= 4500 loss: 0.1610509306192398\n",
      " step= 4500 Evaluate Acc: 0.987\n",
      "echo= 3  step= 4600 loss: 0.04893382638692856\n",
      "echo= 3  step= 4700 loss: 0.05983395501971245\n",
      "echo= 3  step= 4800 loss: 0.03558000549674034\n",
      "echo= 3  step= 4900 loss: 0.014029810205101967\n",
      "echo= 3  step= 5000 loss: 0.024435633793473244\n",
      " step= 5000 Evaluate Acc: 0.9858\n",
      "echo= 3  step= 5100 loss: 0.00010709068737924099\n",
      "echo= 3  step= 5200 loss: 0.020000159740447998\n",
      "echo= 3  step= 5300 loss: 0.011837102472782135\n",
      "echo= 3  step= 5400 loss: 0.000830118777230382\n",
      "echo= 4  step= 0 loss: 0.0007902368088252842\n",
      " step= 0 Evaluate Acc: 0.9896\n",
      "echo= 4  step= 100 loss: 0.07348039746284485\n",
      "echo= 4  step= 200 loss: 0.021083181723952293\n",
      "echo= 4  step= 300 loss: 0.1930372565984726\n",
      "echo= 4  step= 400 loss: 0.03080202266573906\n",
      "echo= 4  step= 500 loss: 2.9239911327749724e-06\n",
      " step= 500 Evaluate Acc: 0.9872\n",
      "echo= 4  step= 600 loss: 3.053628825000487e-05\n",
      "echo= 4  step= 700 loss: 0.00040718610398471355\n",
      "echo= 4  step= 800 loss: 4.395794348965865e-06\n",
      "echo= 4  step= 900 loss: 0.003281869227066636\n",
      "echo= 4  step= 1000 loss: 0.696261465549469\n",
      " step= 1000 Evaluate Acc: 0.9844\n",
      "echo= 4  step= 1100 loss: 0.05039358511567116\n",
      "echo= 4  step= 1200 loss: 0.026732932776212692\n",
      "echo= 4  step= 1300 loss: 0.0003133636782877147\n",
      "echo= 4  step= 1400 loss: 9.238491429641726e-07\n",
      "echo= 4  step= 1500 loss: 0.05629171058535576\n",
      " step= 1500 Evaluate Acc: 0.9868\n",
      "echo= 4  step= 1600 loss: 0.005889526568353176\n",
      "echo= 4  step= 1700 loss: 0.00011047269799746573\n",
      "echo= 4  step= 1800 loss: 0.0001295867405133322\n",
      "echo= 4  step= 1900 loss: 4.560150773613714e-05\n",
      "echo= 4  step= 2000 loss: 0.0737178698182106\n",
      " step= 2000 Evaluate Acc: 0.98\n",
      "echo= 4  step= 2100 loss: 0.009413132444024086\n",
      "echo= 4  step= 2200 loss: 1.3910154848417733e-05\n",
      "echo= 4  step= 2300 loss: 0.00015375754446722567\n",
      "echo= 4  step= 2400 loss: 0.02013564109802246\n",
      "echo= 4  step= 2500 loss: 0.003272288478910923\n",
      " step= 2500 Evaluate Acc: 0.9832\n",
      "echo= 4  step= 2600 loss: 0.0006250393926165998\n",
      "echo= 4  step= 2700 loss: 0.029424596577882767\n",
      "echo= 4  step= 2800 loss: 0.05735519900918007\n",
      "echo= 4  step= 2900 loss: 0.0001260850258404389\n",
      "echo= 4  step= 3000 loss: 0.003447676310315728\n",
      " step= 3000 Evaluate Acc: 0.9862\n",
      "echo= 4  step= 3100 loss: 0.018186606466770172\n",
      "echo= 4  step= 3200 loss: 0.02205575630068779\n",
      "echo= 4  step= 3300 loss: 0.027852509170770645\n",
      "echo= 4  step= 3400 loss: 0.018265845254063606\n",
      "echo= 4  step= 3500 loss: 0.030011290684342384\n",
      " step= 3500 Evaluate Acc: 0.9868\n",
      "echo= 4  step= 3600 loss: 0.0002929803740698844\n",
      "echo= 4  step= 3700 loss: 0.00038144999416545033\n",
      "echo= 4  step= 3800 loss: 2.8271069822949357e-05\n",
      "echo= 4  step= 3900 loss: 0.002487534424290061\n",
      "echo= 4  step= 4000 loss: 0.0004409014945849776\n",
      " step= 4000 Evaluate Acc: 0.985\n",
      "echo= 4  step= 4100 loss: 0.051403969526290894\n",
      "echo= 4  step= 4200 loss: 0.004106947686523199\n",
      "echo= 4  step= 4300 loss: 2.4590897282905644e-06\n",
      "echo= 4  step= 4400 loss: 0.0028309884946793318\n",
      "echo= 4  step= 4500 loss: 1.4681081665912643e-05\n",
      " step= 4500 Evaluate Acc: 0.9876\n",
      "echo= 4  step= 4600 loss: 0.2373618483543396\n",
      "echo= 4  step= 4700 loss: 0.021650278940796852\n",
      "echo= 4  step= 4800 loss: 0.019658591598272324\n",
      "echo= 4  step= 4900 loss: 0.00011895336501765996\n",
      "echo= 4  step= 5000 loss: 0.010377716273069382\n",
      " step= 5000 Evaluate Acc: 0.9834\n",
      "echo= 4  step= 5100 loss: 1.1896194337168708e-05\n",
      "echo= 4  step= 5200 loss: 6.501892494270578e-05\n",
      "echo= 4  step= 5300 loss: 0.08325622230768204\n",
      "echo= 4  step= 5400 loss: 1.4381631444848608e-05\n",
      "echo= 5  step= 0 loss: 0.20417989790439606\n",
      " step= 0 Evaluate Acc: 0.984\n",
      "echo= 5  step= 100 loss: 0.02514120750129223\n",
      "echo= 5  step= 200 loss: 0.2113497108221054\n",
      "echo= 5  step= 300 loss: 0.005074542947113514\n",
      "echo= 5  step= 400 loss: 0.0003724474227055907\n",
      "echo= 5  step= 500 loss: 4.913389147986891e-06\n",
      " step= 500 Evaluate Acc: 0.9864\n",
      "echo= 5  step= 600 loss: 0.0026887902058660984\n",
      "echo= 5  step= 700 loss: 0.0003029739891644567\n",
      "echo= 5  step= 800 loss: 0.03699059039354324\n",
      "echo= 5  step= 900 loss: 0.01143562700599432\n",
      "echo= 5  step= 1000 loss: 0.051685791462659836\n",
      " step= 1000 Evaluate Acc: 0.9864\n",
      "echo= 5  step= 1100 loss: 0.034328069537878036\n",
      "echo= 5  step= 1200 loss: 3.135184556413151e-07\n",
      "echo= 5  step= 1300 loss: 6.10340293860645e-07\n",
      "echo= 5  step= 1400 loss: 1.895332388812676e-06\n",
      "echo= 5  step= 1500 loss: 1.1973059372394346e-05\n",
      " step= 1500 Evaluate Acc: 0.9886\n",
      "echo= 5  step= 1600 loss: 0.0006004077731631696\n",
      "echo= 5  step= 1700 loss: 0.029023347422480583\n",
      "echo= 5  step= 1800 loss: 0.014998487196862698\n",
      "echo= 5  step= 1900 loss: 0.026927011087536812\n",
      "echo= 5  step= 2000 loss: 0.03217855840921402\n",
      " step= 2000 Evaluate Acc: 0.9854\n",
      "echo= 5  step= 2100 loss: 0.0601041205227375\n",
      "echo= 5  step= 2200 loss: 0.036352455615997314\n",
      "echo= 5  step= 2300 loss: 0.027808936312794685\n",
      "echo= 5  step= 2400 loss: 0.07091626524925232\n",
      "echo= 5  step= 2500 loss: 0.01416072342544794\n",
      " step= 2500 Evaluate Acc: 0.9854\n",
      "echo= 5  step= 2600 loss: 0.029614783823490143\n",
      "echo= 5  step= 2700 loss: 2.7632435376290232e-05\n",
      "echo= 5  step= 2800 loss: 0.00991846527904272\n",
      "echo= 5  step= 2900 loss: 0.02307536080479622\n",
      "echo= 5  step= 3000 loss: 2.8378572096698917e-05\n",
      " step= 3000 Evaluate Acc: 0.9868\n",
      "echo= 5  step= 3100 loss: 0.0005470702890306711\n",
      "echo= 5  step= 3200 loss: 0.014772409573197365\n",
      "echo= 5  step= 3300 loss: 0.050184231251478195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo= 5  step= 3400 loss: 0.045525483787059784\n",
      "echo= 5  step= 3500 loss: 0.0015476489206776023\n",
      " step= 3500 Evaluate Acc: 0.9878\n",
      "echo= 5  step= 3600 loss: 0.0008629427757114172\n",
      "echo= 5  step= 3700 loss: 0.002998351352289319\n",
      "echo= 5  step= 3800 loss: 0.02838251367211342\n",
      "echo= 5  step= 3900 loss: 3.158412073389627e-05\n",
      "echo= 5  step= 4000 loss: 0.0002809122088365257\n",
      " step= 4000 Evaluate Acc: 0.9844\n",
      "echo= 5  step= 4100 loss: 0.006299621891230345\n",
      "echo= 5  step= 4200 loss: 0.33633196353912354\n",
      "echo= 5  step= 4300 loss: 0.00015906651969999075\n",
      "echo= 5  step= 4400 loss: 0.0005034782225266099\n",
      "echo= 5  step= 4500 loss: 8.034489269448386e-07\n",
      " step= 4500 Evaluate Acc: 0.9854\n",
      "echo= 5  step= 4600 loss: 0.0009141913615167141\n",
      "echo= 5  step= 4700 loss: 2.7418122883204887e-08\n",
      "echo= 5  step= 4800 loss: 0.00020498395315371454\n",
      "echo= 5  step= 4900 loss: 0.0006785100558772683\n",
      "echo= 5  step= 5000 loss: 0.0009362202836200595\n",
      " step= 5000 Evaluate Acc: 0.987\n",
      "echo= 5  step= 5100 loss: 0.024444343522191048\n",
      "echo= 5  step= 5200 loss: 0.21946343779563904\n",
      "echo= 5  step= 5300 loss: 0.021550260484218597\n",
      "echo= 5  step= 5400 loss: 0.029711322858929634\n",
      "echo= 6  step= 0 loss: 0.004603421315550804\n",
      " step= 0 Evaluate Acc: 0.9848\n",
      "echo= 6  step= 100 loss: 0.027524417266249657\n",
      "echo= 6  step= 200 loss: 0.13675139844417572\n",
      "echo= 6  step= 300 loss: 9.429982128494885e-06\n",
      "echo= 6  step= 400 loss: 0.0011282037012279034\n",
      "echo= 6  step= 500 loss: 0.028170280158519745\n",
      " step= 500 Evaluate Acc: 0.9852\n",
      "echo= 6  step= 600 loss: 0.03941214084625244\n",
      "echo= 6  step= 700 loss: 1.6532973177163512e-06\n",
      "echo= 6  step= 800 loss: 0.02270691841840744\n",
      "echo= 6  step= 900 loss: 1.131037060986273e-05\n",
      "echo= 6  step= 1000 loss: 3.7979047192493454e-05\n",
      " step= 1000 Evaluate Acc: 0.9872\n",
      "echo= 6  step= 1100 loss: 9.857651457423344e-05\n",
      "echo= 6  step= 1200 loss: 0.020798001438379288\n",
      "echo= 6  step= 1300 loss: 0.012329760007560253\n",
      "echo= 6  step= 1400 loss: 0.030577383935451508\n",
      "echo= 6  step= 1500 loss: 0.029413612559437752\n",
      " step= 1500 Evaluate Acc: 0.9838\n",
      "echo= 6  step= 1600 loss: 0.00016559340292587876\n",
      "echo= 6  step= 1700 loss: 0.0010403585620224476\n",
      "echo= 6  step= 1800 loss: 0.01761087216436863\n",
      "echo= 6  step= 1900 loss: 0.002325175330042839\n",
      "echo= 6  step= 2000 loss: 0.02204545959830284\n",
      " step= 2000 Evaluate Acc: 0.985\n",
      "echo= 6  step= 2100 loss: 0.019767338410019875\n",
      "echo= 6  step= 2200 loss: 0.0198545902967453\n",
      "echo= 6  step= 2300 loss: 0.02063917927443981\n",
      "echo= 6  step= 2400 loss: 0.016004489734768867\n",
      "echo= 6  step= 2500 loss: 2.324576229284503e-07\n",
      " step= 2500 Evaluate Acc: 0.9882\n",
      "echo= 6  step= 2600 loss: 0.020449621602892876\n",
      "echo= 6  step= 2700 loss: 0.04920007660984993\n",
      "echo= 6  step= 2800 loss: 0.024578072130680084\n",
      "echo= 6  step= 2900 loss: 2.2814865587861277e-06\n",
      "echo= 6  step= 3000 loss: 0.0002806548145599663\n",
      " step= 3000 Evaluate Acc: 0.9842\n",
      "echo= 6  step= 3100 loss: 0.0021563696209341288\n",
      "echo= 6  step= 3200 loss: 0.025936337187886238\n",
      "echo= 6  step= 3300 loss: 0.0006587766110897064\n",
      "echo= 6  step= 3400 loss: 0.016139943152666092\n",
      "echo= 6  step= 3500 loss: 0.11833125352859497\n",
      " step= 3500 Evaluate Acc: 0.986\n",
      "echo= 6  step= 3600 loss: 8.70518124429509e-05\n",
      "echo= 6  step= 3700 loss: 6.0464302805485204e-05\n",
      "echo= 6  step= 3800 loss: 0.487006276845932\n",
      "echo= 6  step= 3900 loss: 0.04829919710755348\n",
      "echo= 6  step= 4000 loss: 0.24392297863960266\n",
      " step= 4000 Evaluate Acc: 0.9836\n",
      "echo= 6  step= 4100 loss: 2.347087865928188e-05\n",
      "echo= 6  step= 4200 loss: 2.512812898203265e-05\n",
      "echo= 6  step= 4300 loss: 0.05925026908516884\n",
      "echo= 6  step= 4400 loss: 7.9869906244312e-08\n",
      "echo= 6  step= 4500 loss: 0.3546999990940094\n",
      " step= 4500 Evaluate Acc: 0.985\n",
      "echo= 6  step= 4600 loss: 0.0007679163827560842\n",
      "echo= 6  step= 4700 loss: 0.04746439307928085\n",
      "echo= 6  step= 4800 loss: 3.990833283751272e-05\n",
      "echo= 6  step= 4900 loss: 0.03306546062231064\n",
      "echo= 6  step= 5000 loss: 0.06286434829235077\n",
      " step= 5000 Evaluate Acc: 0.9856\n",
      "echo= 6  step= 5100 loss: 1.2566997611429542e-05\n",
      "echo= 6  step= 5200 loss: 0.0010934238089248538\n",
      "echo= 6  step= 5300 loss: 0.04505322128534317\n",
      "echo= 6  step= 5400 loss: 0.0025933010037988424\n",
      "echo= 7  step= 0 loss: 0.0237362552434206\n",
      " step= 0 Evaluate Acc: 0.9866\n",
      "echo= 7  step= 100 loss: 2.2576971332455287e-06\n",
      "echo= 7  step= 200 loss: 7.074326276779175e-05\n",
      "echo= 7  step= 300 loss: 5.4477195590152405e-06\n",
      "echo= 7  step= 400 loss: 0.03464631736278534\n",
      "echo= 7  step= 500 loss: 0.00111970875877887\n",
      " step= 500 Evaluate Acc: 0.9848\n",
      "echo= 7  step= 600 loss: 4.2425348510732874e-05\n",
      "echo= 7  step= 700 loss: 0.04785369709134102\n",
      "echo= 7  step= 800 loss: 0.03237127512693405\n",
      "echo= 7  step= 900 loss: 0.05443139001727104\n",
      "echo= 7  step= 1000 loss: 0.02528093382716179\n",
      " step= 1000 Evaluate Acc: 0.98\n",
      "echo= 7  step= 1100 loss: 0.029262272641062737\n",
      "echo= 7  step= 1200 loss: 0.019554853439331055\n",
      "echo= 7  step= 1300 loss: 0.04749257117509842\n",
      "echo= 7  step= 1400 loss: 0.0014498723903670907\n",
      "echo= 7  step= 1500 loss: 0.028306622058153152\n",
      " step= 1500 Evaluate Acc: 0.9812\n",
      "echo= 7  step= 1600 loss: 0.030399933457374573\n",
      "echo= 7  step= 1700 loss: 0.02608271688222885\n",
      "echo= 7  step= 1800 loss: 0.019457342103123665\n",
      "echo= 7  step= 1900 loss: 0.023648058995604515\n",
      "echo= 7  step= 2000 loss: 0.022405505180358887\n",
      " step= 2000 Evaluate Acc: 0.986\n",
      "echo= 7  step= 2100 loss: 0.03987517207860947\n",
      "echo= 7  step= 2200 loss: 6.103330747464497e-07\n",
      "echo= 7  step= 2300 loss: 0.0013402391923591495\n",
      "echo= 7  step= 2400 loss: 0.03247104957699776\n",
      "echo= 7  step= 2500 loss: 0.04788494482636452\n",
      " step= 2500 Evaluate Acc: 0.9838\n",
      "echo= 7  step= 2600 loss: 5.841246064619554e-08\n",
      "echo= 7  step= 2700 loss: 0.01935952715575695\n",
      "echo= 7  step= 2800 loss: 0.0022623115219175816\n",
      "echo= 7  step= 2900 loss: 0.04819636419415474\n",
      "echo= 7  step= 3000 loss: 1.3351353800317156e-07\n",
      " step= 3000 Evaluate Acc: 0.9856\n",
      "echo= 7  step= 3100 loss: 0.016537075862288475\n",
      "echo= 7  step= 3200 loss: 0.00019192580657545477\n",
      "echo= 7  step= 3300 loss: 0.024934612214565277\n",
      "echo= 7  step= 3400 loss: 0.0003199480415787548\n",
      "echo= 7  step= 3500 loss: 0.04102085903286934\n",
      " step= 3500 Evaluate Acc: 0.987\n",
      "echo= 7  step= 3600 loss: 0.04782608896493912\n",
      "echo= 7  step= 3700 loss: 0.04879326745867729\n",
      "echo= 7  step= 3800 loss: 0.012642321176826954\n",
      "echo= 7  step= 3900 loss: 0.011556634679436684\n",
      "echo= 7  step= 4000 loss: 0.04169740155339241\n",
      " step= 4000 Evaluate Acc: 0.9836\n",
      "echo= 7  step= 4100 loss: 0.019238116219639778\n",
      "echo= 7  step= 4200 loss: 0.1043139100074768\n",
      "echo= 7  step= 4300 loss: 0.033251695334911346\n",
      "echo= 7  step= 4400 loss: 0.00014525750884786248\n",
      "echo= 7  step= 4500 loss: 0.02307722344994545\n",
      " step= 4500 Evaluate Acc: 0.985\n",
      "echo= 7  step= 4600 loss: 8.523101087121177e-07\n",
      "echo= 7  step= 4700 loss: 0.024157634004950523\n",
      "echo= 7  step= 4800 loss: 6.675698216440651e-08\n",
      "echo= 7  step= 4900 loss: 0.5212299227714539\n",
      "echo= 7  step= 5000 loss: 0.02495506964623928\n",
      " step= 5000 Evaluate Acc: 0.9826\n",
      "echo= 7  step= 5100 loss: 9.655907717842638e-08\n",
      "echo= 7  step= 5200 loss: 0.0607551634311676\n",
      "echo= 7  step= 5300 loss: 0.02860601432621479\n",
      "echo= 7  step= 5400 loss: 0.04615379497408867\n",
      "echo= 8  step= 0 loss: 0.021827714517712593\n",
      " step= 0 Evaluate Acc: 0.9822\n",
      "echo= 8  step= 100 loss: 1.813092126212723e-06\n",
      "echo= 8  step= 200 loss: 3.0040607157388877e-07\n",
      "echo= 8  step= 300 loss: 0.032729391008615494\n",
      "echo= 8  step= 400 loss: 0.6826154589653015\n",
      "echo= 8  step= 500 loss: 0.029774438589811325\n",
      " step= 500 Evaluate Acc: 0.9844\n",
      "echo= 8  step= 600 loss: 0.00043377617839723825\n",
      "echo= 8  step= 700 loss: 4.6490541194543766e-07\n",
      "echo= 8  step= 800 loss: 0.00039900458068586886\n",
      "echo= 8  step= 900 loss: 0.028364282101392746\n",
      "echo= 8  step= 1000 loss: 0.031215153634548187\n",
      " step= 1000 Evaluate Acc: 0.984\n",
      "echo= 8  step= 1100 loss: 0.11413802206516266\n",
      "echo= 8  step= 1200 loss: 0.0022271403577178717\n",
      "echo= 8  step= 1300 loss: 3.7896261346759275e-05\n",
      "echo= 8  step= 1400 loss: 0.00016902067000046372\n",
      "echo= 8  step= 1500 loss: 0.012215669266879559\n",
      " step= 1500 Evaluate Acc: 0.986\n",
      "echo= 8  step= 1600 loss: 0.000221210575546138\n",
      "echo= 8  step= 1700 loss: 8.354273631994147e-06\n",
      "echo= 8  step= 1800 loss: 0.04381604120135307\n",
      "echo= 8  step= 1900 loss: 0.011858459562063217\n",
      "echo= 8  step= 2000 loss: 0.08018189668655396\n",
      " step= 2000 Evaluate Acc: 0.9838\n",
      "echo= 8  step= 2100 loss: 0.01966393180191517\n",
      "echo= 8  step= 2200 loss: 0.00038385260268114507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo= 8  step= 2300 loss: 0.00038603157736361027\n",
      "echo= 8  step= 2400 loss: 0.0391092412173748\n",
      "echo= 8  step= 2500 loss: 0.02722027152776718\n",
      " step= 2500 Evaluate Acc: 0.9846\n",
      "echo= 8  step= 2600 loss: 0.08589651435613632\n",
      "echo= 8  step= 2700 loss: 0.02673925645649433\n",
      "echo= 8  step= 2800 loss: 0.039742741733789444\n",
      "echo= 8  step= 2900 loss: 0.035695239901542664\n",
      "echo= 8  step= 3000 loss: 0.01912888139486313\n",
      " step= 3000 Evaluate Acc: 0.9834\n",
      "echo= 8  step= 3100 loss: 0.00010273669613525271\n",
      "echo= 8  step= 3200 loss: 0.033668890595436096\n",
      "echo= 8  step= 3300 loss: 0.006764848716557026\n",
      "echo= 8  step= 3400 loss: 1.0967210073431488e-07\n",
      "echo= 8  step= 3500 loss: 2.412653202554793e-06\n",
      " step= 3500 Evaluate Acc: 0.9844\n",
      "echo= 8  step= 3600 loss: 0.02594275027513504\n",
      "echo= 8  step= 3700 loss: 2.706014470277296e-07\n",
      "echo= 8  step= 3800 loss: 1.5902000086498447e-05\n",
      "echo= 8  step= 3900 loss: 7.03332432294701e-08\n",
      "echo= 8  step= 4000 loss: 0.02248159423470497\n",
      " step= 4000 Evaluate Acc: 0.987\n",
      "echo= 8  step= 4100 loss: 0.03213101997971535\n",
      "echo= 8  step= 4200 loss: 0.06130797415971756\n",
      "echo= 8  step= 4300 loss: 0.00016703043365851045\n",
      "echo= 8  step= 4400 loss: 1.4184900010150159e-06\n",
      "echo= 8  step= 4500 loss: 0.023184234276413918\n",
      " step= 4500 Evaluate Acc: 0.9872\n",
      "echo= 8  step= 4600 loss: 3.3004316719598137e-06\n",
      "echo= 8  step= 4700 loss: 0.030085748061537743\n",
      "echo= 8  step= 4800 loss: 0.029137728735804558\n",
      "echo= 8  step= 4900 loss: 1.19209286886246e-09\n",
      "echo= 8  step= 5000 loss: 0.03462449461221695\n",
      " step= 5000 Evaluate Acc: 0.9858\n",
      "echo= 8  step= 5100 loss: 0.024340782314538956\n",
      "echo= 8  step= 5200 loss: 0.07002739608287811\n",
      "echo= 8  step= 5300 loss: 0.019479384645819664\n",
      "echo= 8  step= 5400 loss: 0.03939811885356903\n",
      "echo= 9  step= 0 loss: 0.0002823293616529554\n",
      " step= 0 Evaluate Acc: 0.9808\n",
      "echo= 9  step= 100 loss: 0.0415022075176239\n",
      "echo= 9  step= 200 loss: 0.09361978620290756\n",
      "echo= 9  step= 300 loss: 0.04980384185910225\n",
      "echo= 9  step= 400 loss: 0.0002389887667959556\n",
      "echo= 9  step= 500 loss: 1.2659224921662826e-06\n",
      " step= 500 Evaluate Acc: 0.9856\n",
      "echo= 9  step= 600 loss: 0.08470997214317322\n",
      "echo= 9  step= 700 loss: 0.029457401484251022\n",
      "echo= 9  step= 800 loss: 0.03114963136613369\n",
      "echo= 9  step= 900 loss: 0.014720619656145573\n",
      "echo= 9  step= 1000 loss: 0.02576727420091629\n",
      " step= 1000 Evaluate Acc: 0.9836\n",
      "echo= 9  step= 1100 loss: 0.00019217803492210805\n",
      "echo= 9  step= 1200 loss: 0.02607615478336811\n",
      "echo= 9  step= 1300 loss: 5.1259863909081105e-08\n",
      "echo= 9  step= 1400 loss: 0.024398520588874817\n",
      "echo= 9  step= 1500 loss: 0.020883841440081596\n",
      " step= 1500 Evaluate Acc: 0.9888\n",
      "echo= 9  step= 1600 loss: 0.18817147612571716\n",
      "echo= 9  step= 1700 loss: 0.9725145101547241\n",
      "echo= 9  step= 1800 loss: 0.03357473015785217\n",
      "echo= 9  step= 1900 loss: 0.8275820016860962\n",
      "echo= 9  step= 2000 loss: 0.02085486613214016\n",
      " step= 2000 Evaluate Acc: 0.9812\n",
      "echo= 9  step= 2100 loss: 0.0\n",
      "echo= 9  step= 2200 loss: 0.0\n",
      "echo= 9  step= 2300 loss: 0.017435969784855843\n",
      "echo= 9  step= 2400 loss: 0.07271713763475418\n",
      "echo= 9  step= 2500 loss: 0.04825067147612572\n",
      " step= 2500 Evaluate Acc: 0.9784\n",
      "echo= 9  step= 2600 loss: 0.3777238130569458\n",
      "echo= 9  step= 2700 loss: 2.3841845830929742e-08\n",
      "echo= 9  step= 2800 loss: 0.024869995191693306\n",
      "echo= 9  step= 2900 loss: 0.021966945379972458\n",
      "echo= 9  step= 3000 loss: 0.028245458379387856\n",
      " step= 3000 Evaluate Acc: 0.9854\n",
      "echo= 9  step= 3100 loss: 0.03061341494321823\n",
      "echo= 9  step= 3200 loss: 0.05431525409221649\n",
      "echo= 9  step= 3300 loss: 0.020069031044840813\n",
      "echo= 9  step= 3400 loss: 6.735522219969425e-06\n",
      "echo= 9  step= 3500 loss: 0.018823212012648582\n",
      " step= 3500 Evaluate Acc: 0.9832\n",
      "echo= 9  step= 3600 loss: 0.0\n",
      "echo= 9  step= 3700 loss: 0.333908349275589\n",
      "echo= 9  step= 3800 loss: 0.01698087714612484\n",
      "echo= 9  step= 3900 loss: 0.041645947843790054\n",
      "echo= 9  step= 4000 loss: 0.02088918164372444\n",
      " step= 4000 Evaluate Acc: 0.9838\n",
      "echo= 9  step= 4100 loss: 0.019870586693286896\n",
      "echo= 9  step= 4200 loss: 0.014918860048055649\n",
      "echo= 9  step= 4300 loss: 0.04489537328481674\n",
      "echo= 9  step= 4400 loss: 0.06034605950117111\n",
      "echo= 9  step= 4500 loss: 0.0\n",
      " step= 4500 Evaluate Acc: 0.9814\n",
      "echo= 9  step= 4600 loss: 7.868398097343743e-05\n",
      "echo= 9  step= 4700 loss: 0.0\n",
      "echo= 9  step= 4800 loss: 0.09040096402168274\n",
      "echo= 9  step= 4900 loss: 0.07048171013593674\n",
      "echo= 9  step= 5000 loss: 4.76837058727142e-09\n",
      " step= 5000 Evaluate Acc: 0.9794\n",
      "echo= 9  step= 5100 loss: 0.06567687541246414\n",
      "echo= 9  step= 5200 loss: 0.018951570615172386\n",
      "echo= 9  step= 5300 loss: 0.03275949880480766\n",
      "echo= 9  step= 5400 loss: 0.0033301732037216425\n",
      "train is over\n"
     ]
    }
   ],
   "source": [
    "acclist=[]\n",
    "losslist=[]\n",
    "echos=[1,2,3,4,5,6,7,8,9,10]\n",
    "#分批进行训练\n",
    "for echo in range(10):\n",
    "    for step, (x,y) in enumerate(db):#从训练集读取一批样本数据\n",
    "        with tf.GradientTape() as tape:#构建梯度训练环境\n",
    "            # [b, 28, 28] => [b, 784]\n",
    "            x = tf.expand_dims(x,axis=3)\n",
    "            # [b, 784] => [b, 10]\n",
    "            out = network(x,training=True)\n",
    "            # [b] => [b, 10]\n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "            # [b]\n",
    "            #计算交叉熵损失函数\n",
    "            loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n",
    "\n",
    "        grads = tape.gradient(loss, network.trainable_variables)#计算梯度\n",
    "        optimizer.apply_gradients(zip(grads, network.trainable_variables))#更新训练参数\n",
    "        if step % 100 == 0:\n",
    "            print('echo=',echo,' step=',step, 'loss:', float(loss))#打印训练的损失函数\n",
    "        \n",
    "# 模型评价\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            total, total_correct = 0., 0\n",
    "            for i, (x, y) in enumerate(ds_val):\n",
    "                # [b, 28, 28] => [b, 784]\n",
    "                x = tf.expand_dims(x,axis=3)\n",
    "                # [b, 784] => [b, 10]\n",
    "                out = network(x)#神经网络模型输出\n",
    "                # [b, 10] => [b]\n",
    "                pred = tf.argmax(out, axis=1)\n",
    "                pred = tf.cast(pred, dtype=tf.int32)\n",
    "                # bool type\n",
    "                correct = tf.equal(pred, y)\n",
    "                # bool tensor => int tensor => numpy\n",
    "                total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n",
    "                total += x.shape[0]\n",
    "            print(' step=',step, 'Evaluate Acc:', total_correct/total)#打印模型验证正确率\n",
    "        if step  == 5000:       \n",
    "            acclist.append(total_correct/total)\n",
    "            losslist.append(float(loss))\n",
    "print(\"train is over\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(losslist)\n",
    "#print(acclist)\n",
    "plt.figure()\n",
    "plt.plot(echos, acclist,'d-',lw=2)\n",
    "plt.xlabel('echos')\n",
    "plt.ylabel('acc')\n",
    "plt.xticks(echos,fontsize=8)\n",
    "plt.savefig('convolution1.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(echos, losslist,'d-',lw=2)\n",
    "plt.xlabel('echos')\n",
    "plt.ylabel('loss')\n",
    "plt.xticks(echos,fontsize=8)\n",
    "plt.savefig('convolution2.png', dpi=300)#\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试模型\n",
    "total, total_correct = 0., 0\n",
    "for i, (x, y) in enumerate(db_test):#读取一批测试数据\n",
    "    # [b, 28, 28] => [b, 784]\n",
    "    x = tf.expand_dims(x,axis=3)\n",
    "    # [b, 784] => [b, 10]\n",
    "    out = network(x)#神经网络模型输出\n",
    "    # [b, 10] => [b]\n",
    "    pred = tf.argmax(out, axis=1)\n",
    "    pred = tf.cast(pred, dtype=tf.int32)\n",
    "    # bool type\n",
    "    correct = tf.equal(pred, y)\n",
    "    # bool tensor => int tensor => numpy\n",
    "    btestacc=tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n",
    "    total_correct += btestacc\n",
    "    total += x.shape[0]\n",
    "    #print('第',i,'批','test acc=',btestacc/x.shape[0])\n",
    "    #print(y)\n",
    "    #print(pred)\n",
    "print('Test Acc:', total_correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
