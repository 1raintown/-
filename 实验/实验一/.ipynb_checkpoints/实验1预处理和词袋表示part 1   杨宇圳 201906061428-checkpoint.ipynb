{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 align=center><font size = 5> <center>文本分析与挖掘</center> </font></h1> \n",
    "\n",
    "<h2 align=center><font size = 4><center>实验一、文本预处理和基本表示-Part 1</center></font></h2>\n",
    "<h2 align=center><font size = 2><center>浙江工业大学计算机科学与技术学院 杨宇圳</center></font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验内容\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<li>英文常用预处理方法</li>\n",
    "<li>中文分词</li>\n",
    "<li>分句</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> 1. 英文预处理</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>a.导入包</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "ps=PorterStemmer()#词干提取\n",
    "wnl = WordNetLemmatizer() #词性还原\n",
    "stopwords_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>b.编写英文预处理函数 EngPreprocess(),对输入的一个英文段落实现以下功能：分词、词干提取、词性还原、去停用词。</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):#获取词性\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def remove_stopwords (text):#去停用词\n",
    "    filtered_word=[i for i in text if i.lower() not in stopwords_list ]\n",
    "    filtered_text=' '.join(filtered_word)\n",
    "    return filtered_text    \n",
    "\n",
    "#编写函数EngPreprocess\n",
    "def EngPreprocess(input_text, tokenization=False,\n",
    "                  stemming=False, lemmatization=False,stopword_remove=False):\n",
    "    if tokenization :#分词\n",
    "        input_text=word_tokenize(input_text)\n",
    "    if stemming :#词干提取\n",
    "        input_text=word_tokenize(input_text)\n",
    "        input_text=[ps.stem(i) for i in input_text]\n",
    "        input_text=' '.join(input_text)\n",
    "    if lemmatization:#词性还原\n",
    "        lemmas_sent = []\n",
    "        input_text=word_tokenize(input_text)\n",
    "        tagged_sent = pos_tag(input_text)     # 获取单词词性\n",
    "        for tag in tagged_sent:\n",
    "            wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "            lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos))# 词形还原\n",
    "            \n",
    "        input_text=' '.join(lemmas_sent)\n",
    "    if stopword_remove:#去停用词\n",
    "        input_text=word_tokenize(input_text)\n",
    "        input_text=remove_stopwords(input_text)\n",
    "        \n",
    "    return input_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>c.对比text.split()与分词结果；</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用以下几个句子作为测试：\n",
    "<li>句子1：UK coronavirus cases top 600,000 as another 12,872 confirmed</li>\n",
    "<li>句子2：Global COVID-19 cases surpass 38 mln: Johns Hopkins University</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样例一： UK coronavirus cases top 600,000 as another 12,872 confirmed\n",
      "EngPreprocess()结果1: ['UK', 'coronavirus', 'cases', 'top', '600,000', 'as', 'another', '12,872', 'confirmed']\n",
      "text.split()结果1: ['UK', 'coronavirus', 'cases', 'top', '600,000', 'as', 'another', '12,872', 'confirmed']\n",
      "\n",
      "样例二： Global COVID-19 cases surpass 38 mln: Johns Hopkins University\n",
      "EngPreprocess()结果2: ['Global', 'COVID-19', 'cases', 'surpass', '38', 'mln', ':', 'Johns', 'Hopkins', 'University']\n",
      "text.split()结果2: ['Global', 'COVID-19', 'cases', 'surpass', '38', 'mln:', 'Johns', 'Hopkins', 'University']\n"
     ]
    }
   ],
   "source": [
    "#对以上例句分别调用EngPreprocess()中的分词功能，text.split()得到结果\n",
    "\n",
    "input_text=\"UK coronavirus cases top 600,000 as another 12,872 confirmed\"\n",
    "print(\"样例一：\",input_text)\n",
    "test_text1=EngPreprocess(input_text, tokenization=True)\n",
    "print('EngPreprocess()结果1:',test_text1)\n",
    "print('text.split()结果1:',input_text.split())\n",
    "print()\n",
    "input_text=\"Global COVID-19 cases surpass 38 mln: Johns Hopkins University\"\n",
    "print(\"样例二：\",input_text)\n",
    "test_text1=EngPreprocess(input_text, tokenization=True)\n",
    "print('EngPreprocess()结果2:',test_text1)\n",
    "print('text.split()结果2:',input_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 10px\">\n",
    "结果分析和讨论：自定义的分词函数与text.split()的结果大致相似，而text.split()只会根据空格来分词，“mln:”这里就未分开，而自定义的分词函数则能很好地完成任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>d.对词干提取、词性还原、去停用词分别进行测试（用多个不同样例），观察结果并讨论准确性。</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用以下样例测试词干提取功能\n",
    "<li>jumping</li>\n",
    "<li>jumps</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样例一： jumping\n",
      "jump\n",
      "样例二： jumps\n",
      "jump\n"
     ]
    }
   ],
   "source": [
    "input_text=\"jumping\"\n",
    "print(\"样例一：\",input_text)\n",
    "print(EngPreprocess(input_text, stemming=True))\n",
    "\n",
    "input_text=\"jumps\"\n",
    "print(\"样例二：\",input_text)\n",
    "print(EngPreprocess(input_text, stemming=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 10px\">\n",
    "结果分析和讨论：查看结果，发现可以实现对词干提取功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用以下样例测试词性还原功能\n",
    "<li>There are a lot of cars</li>\n",
    "<li>Whatever is worth doing is worth doing well.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样例一： There are a lot of cars\n",
      "There be a lot of car\n",
      "样例二： Whatever is worth doing is worth doing well.\n",
      "Whatever be worth do be worth do well .\n"
     ]
    }
   ],
   "source": [
    "input_text=\"There are a lot of cars\"\n",
    "print(\"样例一：\",input_text)\n",
    "print(EngPreprocess(input_text, lemmatization=True))\n",
    "input_text=\"Whatever is worth doing is worth doing well.\"\n",
    "print(\"样例二：\",input_text)\n",
    "print(EngPreprocess(input_text, lemmatization=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 10px\">\n",
    "结果分析和讨论：对照结果，发现可以实现词性还原的功能，且准确性较高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用以下样例测试去停用词功能\n",
    "<li>The,and,if are stopwords,computer is not</li>\n",
    "<li>Love is not a maybe thing. You know when you love someone.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样例一： The,and,if are stopwords,computer is not\n",
      ", , stopwords , computer\n",
      "样例二： Love is not a maybe thing. You know when you love someone.\n",
      "Love maybe thing . know love someone .\n"
     ]
    }
   ],
   "source": [
    "input_text=\"The,and,if are stopwords,computer is not\"\n",
    "print(\"样例一：\",input_text)\n",
    "print(EngPreprocess(input_text,stopword_remove=True))\n",
    "input_text=\"Love is not a maybe thing. You know when you love someone.\"\n",
    "print(\"样例二：\",input_text)\n",
    "print(EngPreprocess(input_text,stopword_remove=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 10px\">\n",
    "结果分析和讨论：对照结果，发现可以实现去停用词的功能，且准确性较高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> 2. 中文分词</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.编写函数 ChTokenize(),基于 jieba 实现中文分词。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChTokenize(input_text,cut_all=False):\n",
    "    seg_list=jieba.cut(input_text,cut_all)\n",
    "    return seg_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用以下样例测试中文分词功能\n",
    "<li>人生就是一场旅行，不在乎目的地，在乎的应该是沿途的风景以及看风景的心情。</li>\n",
    "<li>用以下样例测试中文分词功能。</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\DELL\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样例一： 人生就是一场旅行，不在乎目的地，在乎的应该是沿途的风景以及看风景的心情。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.568 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精确模式： 人生/就是/一场/旅行/，/不在乎/目的地/，/在乎/的/应该/是/沿途/的/风景/以及/看/风景/的/心情/。\n",
      "全模式： 人生/生就/就是/一场/旅行/，/不在/不在乎/在乎/目的/目的地/，/在乎/的/应该/该是/沿途/的/风景/以及/看/风景/的/心情/。\n",
      "\n",
      "样例二： 用以下样例测试中文分词功能。\n",
      "精确模式： 用/以下/样例/测试/中文/分词/功能/。\n",
      "全模式： 用以/以下/样/例/测试/中文/分词/功能/。\n"
     ]
    }
   ],
   "source": [
    "input_text=\"人生就是一场旅行，不在乎目的地，在乎的应该是沿途的风景以及看风景的心情。\"\n",
    "print(\"样例一：\",input_text)\n",
    "print(u\"精确模式：\",'/'.join(ChTokenize(input_text)))\n",
    "print(u\"全模式：\",'/'.join(ChTokenize(input_text,True)))\n",
    "print()\n",
    "\n",
    "input_text=\"用以下样例测试中文分词功能。\"\n",
    "print(\"样例二：\",input_text)\n",
    "print(u\"精确模式：\",'/'.join(ChTokenize(input_text)))\n",
    "print(u\"全模式：\",'/'.join(ChTokenize(input_text,True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 10px\">\n",
    "结果分析和讨论：精确模式下，分词的结果较符合语义的分词，而全模式下则将全部可能出现的词都进行了分隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.分句"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.编写函数 Doc2Sent(),实现对英文、中文文档进行分句。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def merge_symmetry(sentences, symmetry=('“','”')):\n",
    "    '''中文拼接对称符号'''\n",
    "    effective_ = []\n",
    "    merged = True\n",
    "    for index in range(len(sentences)):       \n",
    "        if symmetry[0] in sentences[index] and symmetry[1] not in sentences[index]:\n",
    "            merged = False\n",
    "            effective_.append(sentences[index])\n",
    "        elif symmetry[1] in sentences[index] and not merged:\n",
    "            merged = True\n",
    "            effective_[-1] += sentences[index]\n",
    "        elif symmetry[0] not in sentences[index] and symmetry[1] not in sentences[index] and not merged :\n",
    "            effective_[-1] += sentences[index]\n",
    "        else:\n",
    "            effective_.append(sentences[index])\n",
    "        \n",
    "    return [i.strip() for i in effective_ if len(i.strip()) > 0]\n",
    "\n",
    "def Doc2Sent(paragraph,mode):\n",
    "    if mode=='Chinese':\n",
    "        \"\"\"中文分句\"\"\"\n",
    "        sentences = re.split(r\"(？|。|！|\\…\\…)\", paragraph)\n",
    "        \n",
    "        sentences.append(\"\")\n",
    "        sentences = [\"\".join(i) for i in zip(sentences[0::2], sentences[1::2])]\n",
    "        sentences = [i.strip() for i in sentences if len(i.strip()) > 0]\n",
    "        #进行 ” 的拼接\n",
    "        for j in range(1, len(sentences)):\n",
    "            if sentences[j][0] == '”':\n",
    "                sentences[j-1] = sentences[j-1] + '”'\n",
    "                sentences[j] = sentences[j][1:]\n",
    "\n",
    "        return merge_symmetry(sentences)\n",
    "    elif mode==\"English\":\n",
    "        '''英文分句'''\n",
    "        sent_tokenize_list = sent_tokenize(paragraph)\n",
    "        \n",
    "        return sent_tokenize_list\n",
    "    else :\n",
    "        print(\"模式选择有误，重新输入\")\n",
    "        return \"请重新输入\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文样段：\n",
    "\n",
    "&emsp;&emsp;这问题很简单。因为如果泛指“人类”，那就是站在生物的立场。结婚，是为了传宗接代。虽然说，不结婚而同居或只发生性的关系，照样可以传宗接代。但那样实在对所要传的后代很不方便。因为数千年来，人类根据经验，已经得到证明，要安全地抚养后代，实在不是单单的男方或女方独自的力量可做得好的，而必须双方分工合作，有人在家照顾哺育，有人出外打工谋生，才不会顾此失彼，疲于奔命。所以双方要建立一个固定的居所，有个固定的名分，负起人伦、社会与法律上的责任，使这关系巩固而公开，以免中途发生动摇或受到外力的侵害。这样才可以有效地保护及教养子女，使他们成为人类所希求的、良好的后代。所以，“人”需要结婚!\n",
    "    \n",
    "    \n",
    "英文样段：\n",
    "\n",
    "I remember quite clearly now when the story happened. The autumn leaves were floating in measure down to the ground, recovering the lake, where we used to swim like children, under the sun was there to shine. That time we used to be happy. Well, I thought we were. But the truth was that you had been longing to leave me, not daring to tell me. On that precious night, watching the lake, vaguely conscious, you said: Our story is ending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文分句：\n",
      "这问题很简单。\n",
      "因为如果泛指，我说“人类。就是如此。”\n",
      "那就是站在生物的立场。\n",
      "结婚，是为了传宗接代。\n",
      "虽然说，不结婚而同居或只发生性的关系，照样可以传宗接代。\n",
      "但那样实在对所要传的后代很不方便。\n",
      "因为数千年来，人类根据经验，已经得到证明，要安全地抚养后代，实在不是单单的男方或女方独自的力量可做得好的，而必须双方分工合作，有人在家照顾哺育，有人出外打工谋生，才不会顾此失彼，疲于奔命。\n",
      "所以双方要建立一个固定的居所，有个固定的名分，负起人伦、社会与法律上的责任，使这关系巩固而公开，以免中途发生动摇或受到外力的侵害。\n",
      "这样才可以有效地保护及教养子女，使他们成为人类所希求的、良好的后代。\n",
      "所以，“人”需要结婚!\n",
      "\n",
      "英文分句：\n",
      "I remember quite clearly now when the story happened......\n",
      "The autumn leaves were floating in measure down to the ground, recovering the lake, where we used to swim like children, under the sun was there to shine.\n",
      "That time we used to be happy.\n",
      "Well, I thought we were.\n",
      "But the truth was that you had been longing to leave me, not daring to tell me.\n",
      "On that precious night, watching the lake, vaguely conscious, you said: Our story is ending.\n"
     ]
    }
   ],
   "source": [
    "C_paragraph=\"这问题很简单。因为如果泛指，我说“人类。就是如此。”那就是站在生物的立场。结婚，是为了传宗接代。虽然说，不结婚而同居或只发生性的关系，照样可以传宗接代。但那样实在对所要传的后代很不方便。因为数千年来，人类根据经验，已经得到证明，要安全地抚养后代，实在不是单单的男方或女方独自的力量可做得好的，而必须双方分工合作，有人在家照顾哺育，有人出外打工谋生，才不会顾此失彼，疲于奔命。所以双方要建立一个固定的居所，有个固定的名分，负起人伦、社会与法律上的责任，使这关系巩固而公开，以免中途发生动摇或受到外力的侵害。这样才可以有效地保护及教养子女，使他们成为人类所希求的、良好的后代。所以，“人”需要结婚!\"\n",
    "print('中文分句：')\n",
    "print('\\n'.join(Doc2Sent(C_paragraph,'Chinese')))\n",
    "print()\n",
    "E_paragraph=\"I remember quite clearly now when the story happened...... The autumn leaves were floating in measure down to the ground, recovering the lake, where we used to swim like children, under the sun was there to shine. That time we used to be happy. Well, I thought we were. But the truth was that you had been longing to leave me, not daring to tell me. On that precious night, watching the lake, vaguely conscious, you said: Our story is ending.\"\n",
    "print('英文分句：')\n",
    "print('\\n'.join(Doc2Sent(E_paragraph,'English')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 10px\">\n",
    "结果分析和讨论：中文段落和英文段落都准确地进行了分句"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
