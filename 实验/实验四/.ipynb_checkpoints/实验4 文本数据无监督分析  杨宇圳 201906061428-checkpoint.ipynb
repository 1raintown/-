{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 align=center><font size = 5> <center>文本分析与挖掘</center> </font></h1> \n",
    "\n",
    "<h2 align=center><font size = 4><center>实验四、文本数据无监督分析</center></font></h2>\n",
    "<h2 align=center><font size = 2><center>浙江工业大学计算机科学与技术学院</center></font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、实验目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<li>对文本数据进行聚类，对比不同预处理和表示对聚类结果有效性的影响。</li>\n",
    "<li>熟悉无标签数据的聚类分析过程。</li>\n",
    "<li>熟悉文本主题检测方法和结果，对比不同方法的有效性。 </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实验内容\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<li>用 k-means 算法对新闻数据进行聚类和评估</li>\n",
    "<li>主题检测和方法对比</li>\n",
    "<li>主题级别特征对分类的影响（选做）</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.用 k-means 算法对新闻数据进行聚类和评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.从 20newsgroups 中抽取两个子集 multi3 和 multi5，其中multi3 包含 comp.graphics、rec.autos、talk.politics.guns三个类别，multi5 包含 comp.graphics、comp.windows.x、rec.sport.hockey、rec.autos、talk.politics.guns 五个类别。每个类别随机采样 200 个样本，所以 multi3 共有 600 个文档，multi5 共有 1000 个文档。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "data=pd.read_csv('./data.csv')\n",
    "multi3_1=data[data['Target Name']=='comp.graphics'].sample(200)\n",
    "multi3_2=data[data['Target Name']=='rec.autos'].sample(200)\n",
    "multi3_3=data[data['Target Name']=='talk.politics.guns'].sample(200)\n",
    "\n",
    "multi3=multi3_1.append(multi3_2).append(multi3_3)\n",
    "\n",
    "\n",
    "multi5_1=data[data['Target Name']=='comp.graphics'].sample(200)\n",
    "multi5_2=data[data['Target Name']=='comp.windows.x'].sample(200)\n",
    "multi5_3=data[data['Target Name']=='rec.sport.hockey'].sample(200)\n",
    "multi5_4=data[data['Target Name']=='rec.autos'].sample(200)\n",
    "multi5_5=data[data['Target Name']=='talk.politics.guns'].sample(200)\n",
    "\n",
    "multi5=multi5_1.append(multi5_2).append(multi5_3).append(multi5_4).append(multi5_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. 进行适当预处理后基于 TF-IDF 表示，对 multi3 和 multi5 数据集用 k-means 聚类（k 分别取 3 和 5），得到 10 次结果的平均NMI（归一化互信息）。注意：聚类时用全部数据集，不用分训练和测试。观察讨论数据集分布和类别个数是否会增加聚类难度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi3_corpus=np.array(multi3['Clean Article'])\n",
    "multi5_corpus=np.array(multi5['Clean Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv=TfidfVectorizer(min_df=0.0,max_df=1.0)\n",
    "tv_multi3=tv.fit_transform(multi3_corpus)\n",
    "tv_multi5=tv.fit_transform(multi5_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_3=KMeans(n_clusters=3,max_iter=1000,n_init=10,random_state=42).fit(tv_multi3)\n",
    "\n",
    "km_5=KMeans(n_clusters=5,max_iter=1000,n_init=10,random_state=42).fit(tv_multi5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Clean Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Concerning the proposed newsgroup split, I per...</td>\n",
       "      <td>concern propos newsgroup split , person favor ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7180</th>\n",
       "      <td>\\n   Path: dime!ymir.cs.umass.edu!nic.umass.ed...</td>\n",
       "      <td>path : dime ! ymir.cs.umass.edu ! nic.umass.ed...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4724</th>\n",
       "      <td>\\n[ description of experiment deleted ]\\n\\n\\n[...</td>\n",
       "      <td>[ descript experi delet ] [ delet ] year ago f...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11128</th>\n",
       "      <td>Hi there,\\n\\nI'm looking for help on hi-rez CG...</td>\n",
       "      <td>Hi , ' look help hi-rez cga mode ( hey , know ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7734</th>\n",
       "      <td>Archive-name: graphics/resources-list/part1\\nL...</td>\n",
       "      <td>archive-nam : graphics/resources-list/part1 la...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16909</th>\n",
       "      <td>\\nAs I understand it was considered unsafe for...</td>\n",
       "      <td>understand wa consid unsaf tv network get ani ...</td>\n",
       "      <td>16</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>\\n\\n\\tCan you say, \"I get more background radi...</td>\n",
       "      <td>&amp;#9 ; say , \" get background radiat live denve...</td>\n",
       "      <td>16</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>\\nThat doesn't count as getting rid of the pre...</td>\n",
       "      <td>' count get rid press. get rid press would mea...</td>\n",
       "      <td>16</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7492</th>\n",
       "      <td>\\tI keep hearing people say this.  It assumes ...</td>\n",
       "      <td>&amp;#9 ; keep hear peopl say this. assum , point ...</td>\n",
       "      <td>16</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7006</th>\n",
       "      <td>\\nHere goes:\\n\\nMore than a few years back (if...</td>\n",
       "      <td>goe : year back ( born year , legal drink ) , ...</td>\n",
       "      <td>16</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Article  ... label\n",
       "925    Concerning the proposed newsgroup split, I per...  ...     2\n",
       "7180   \\n   Path: dime!ymir.cs.umass.edu!nic.umass.ed...  ...     0\n",
       "4724   \\n[ description of experiment deleted ]\\n\\n\\n[...  ...     0\n",
       "11128  Hi there,\\n\\nI'm looking for help on hi-rez CG...  ...     0\n",
       "7734   Archive-name: graphics/resources-list/part1\\nL...  ...     0\n",
       "...                                                  ...  ...   ...\n",
       "16909  \\nAs I understand it was considered unsafe for...  ...     2\n",
       "1665   \\n\\n\\tCan you say, \"I get more background radi...  ...     2\n",
       "602    \\nThat doesn't count as getting rid of the pre...  ...     2\n",
       "7492   \\tI keep hearing people say this.  It assumes ...  ...     2\n",
       "7006   \\nHere goes:\\n\\nMore than a few years back (if...  ...     2\n",
       "\n",
       "[600 rows x 5 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi3['label']=km_3.labels_\n",
    "true_label_3=multi3['Target Label']\n",
    "multi3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真实 Counter({1: 200, 5: 200, 10: 200, 7: 200, 16: 200})\n",
      "预测 Counter({1: 465, 0: 208, 4: 200, 2: 105, 3: 22})\n"
     ]
    }
   ],
   "source": [
    "true_label_5=multi5['Target Label']\n",
    "print(\"真实\",Counter(true_label_5))\n",
    "print('预测',Counter(km_5.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 10px\">\n",
    "结果分析和讨论：\n",
    "    \n",
    "对于multl3,计算得到label与真实label对应关系：,0对应1,1对应7，2对应16\n",
    "    \n",
    "对于multl5,计算得到label与真实label对应关系：,1对应1,0对应5，4对应10,2对应7，3对应16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "km_3_label=km_3.labels_.tolist()\n",
    "km_5_label=km_5.labels_.tolist()\n",
    "true_label_3_trans=[]\n",
    "true_label_5_trans=[]\n",
    "#转换真实label\n",
    "for i in true_label_3:\n",
    "    if i==1:\n",
    "        true_label_3_trans.append(0)\n",
    "    elif i==7:\n",
    "        true_label_3_trans.append(1)\n",
    "    else :true_label_3_trans.append(2)\n",
    "\n",
    "for i in true_label_5:\n",
    "    if i==1:\n",
    "        true_label_5_trans.append(1)\n",
    "    elif i==5:\n",
    "        true_label_5_trans.append(0)\n",
    "    elif i==10:\n",
    "        true_label_5_trans.append(4)\n",
    "    elif i==7:\n",
    "        true_label_5_trans.append(2)\n",
    "    else :true_label_5_trans.append(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi3: 0.5083710111305293\n",
      "multi5: 0.2665777149713708\n"
     ]
    }
   ],
   "source": [
    "multi3_NMI= metrics.normalized_mutual_info_score(true_label_3_trans, km_3_label)\n",
    "multi5_NMI= metrics.normalized_mutual_info_score(true_label_5_trans, km_5_label)\n",
    "print('multi3:',multi3_NMI)\n",
    "print('multi5:',multi5_NMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 10px\">\n",
    "结果分析和讨论：multi3的NMI值较高，可见，数据集越大，类别越多，聚类的难度也越大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.改变实验设置（可以是预处理、表示、聚类算法等），对比结果并讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1）使用原始数据进行重复实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi3: 0.15089105631155558\n",
      "multi5: 0.1580663479333953\n"
     ]
    }
   ],
   "source": [
    "multi3_corpus=np.array(multi3['Article'])\n",
    "multi5_corpus=np.array(multi5['Article'])\n",
    "tv=TfidfVectorizer(min_df=0.0,max_df=1.0)\n",
    "tv_multi3=tv.fit_transform(multi3_corpus)\n",
    "tv_multi5=tv.fit_transform(multi5_corpus)\n",
    "km_3=KMeans(n_clusters=3,max_iter=1000,n_init=10,random_state=42).fit(tv_multi3)\n",
    "km_5=KMeans(n_clusters=5,max_iter=1000,n_init=10,random_state=42).fit(tv_multi5)\n",
    "\n",
    "true_label_3=multi3['Target Label']\n",
    "true_label_5=multi5['Target Label']\n",
    "\n",
    "km_3_label=km_3.labels_.tolist()\n",
    "km_5_label=km_5.labels_.tolist()\n",
    "true_label_3_trans=[]\n",
    "true_label_5_trans=[]\n",
    "#转换真实label\n",
    "for i in true_label_3:\n",
    "    if i==1:\n",
    "        true_label_3_trans.append(0)\n",
    "    elif i==7:\n",
    "        true_label_3_trans.append(1)\n",
    "    else :true_label_3_trans.append(2)\n",
    "\n",
    "for i in true_label_5:\n",
    "    if i==1:\n",
    "        true_label_5_trans.append(1)\n",
    "    elif i==5:\n",
    "        true_label_5_trans.append(0)\n",
    "    elif i==10:\n",
    "        true_label_5_trans.append(4)\n",
    "    elif i==7:\n",
    "        true_label_5_trans.append(2)\n",
    "    else :true_label_5_trans.append(3)\n",
    "\n",
    "        \n",
    "multi3_NMI= metrics.normalized_mutual_info_score(true_label_3_trans, km_3_label)\n",
    "multi5_NMI= metrics.normalized_mutual_info_score(true_label_5_trans, km_5_label)\n",
    "print('multi3:',multi3_NMI)\n",
    "print('multi5:',multi5_NMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 10px\">\n",
    "结果分析和讨论：使用不进行预处理的原始数据进行实验，可以发现，两个数据集的NMI值只有0.15左右，聚类的效果不佳，可见预处理的处理是十分必要的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2）使用词频进行表示重复试验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi3: 0.009738562500813014\n",
      "multi5: 0.013631308616282489\n"
     ]
    }
   ],
   "source": [
    "multi3_corpus=np.array(multi3['Clean Article'])\n",
    "multi5_corpus=np.array(multi5['Clean Article'])\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(min_df=0.0,max_df=1.0)\n",
    "\n",
    "cv_multi3=cv.fit_transform(multi3_corpus)\n",
    "cv_multi5=cv.fit_transform(multi5_corpus)\n",
    "\n",
    "km_3=KMeans(n_clusters=3,max_iter=1000,n_init=10,random_state=42).fit(cv_multi3)\n",
    "km_5=KMeans(n_clusters=5,max_iter=1000,n_init=10,random_state=42).fit(cv_multi5)\n",
    "\n",
    "true_label_3=multi3['Target Label']\n",
    "true_label_5=multi5['Target Label']\n",
    "\n",
    "km_3_label=km_3.labels_.tolist()\n",
    "\n",
    "km_5_label=km_5.labels_.tolist()\n",
    "\n",
    "true_label_3_trans=[]\n",
    "true_label_5_trans=[]\n",
    "#转换真实label\n",
    "for i in true_label_3:\n",
    "    if i==1:\n",
    "        true_label_3_trans.append(0)\n",
    "    elif i==7:\n",
    "        true_label_3_trans.append(1)\n",
    "    else :true_label_3_trans.append(2)\n",
    "\n",
    "for i in true_label_5:\n",
    "    if i==1:\n",
    "        true_label_5_trans.append(1)\n",
    "    elif i==5:\n",
    "        true_label_5_trans.append(0)\n",
    "    elif i==10:\n",
    "        true_label_5_trans.append(4)\n",
    "    elif i==7:\n",
    "        true_label_5_trans.append(2)\n",
    "    else :true_label_5_trans.append(3)\n",
    "\n",
    "        \n",
    "multi3_NMI= metrics.normalized_mutual_info_score(true_label_3_trans, km_3_label)\n",
    "multi5_NMI= metrics.normalized_mutual_info_score(true_label_5_trans, km_5_label)\n",
    "print('multi3:',multi3_NMI)\n",
    "print('multi5:',multi5_NMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 10px\">\n",
    "结果分析和讨论：预处理后使用词频进行表示并进行实验，可以发现，两个数据集的NMI值特别低，不存在聚类的效果，由此可见tf-idf的表示十分必要。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.主题检测和方法对比\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. 对 multi5 数据集进行预处理和词袋表示后，用 LDA 进行主题检测：设置主题数目 topic_num=5,打印每个主题 top-10 关键词,根据得到的结果适当调整预处理和表示模型。观察并定性讨论检测到的主题、计算主题的 coherence 分数、以及每个主题在文档中的分布情况（每个文档关联到最相关的主题，再打印主题包含的文档百分比。参考课件：对主题模型的解释和分析）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "norm_corpus=[]\n",
    "clean_article=multi5['Clean Article'].tolist()\n",
    "for i in clean_article:\n",
    "    a=word_tokenize(i)\n",
    "    norm_corpus.append(a)\n",
    "\n",
    "labels=multi5['Target Label'].tolist()\n",
    "\n",
    "dictionary=gensim.corpora.Dictionary(norm_corpus)\n",
    "bow_corpus=[dictionary.doc2bow(text) for text in norm_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "article=multi5['Article'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.061*\",\" + 0.058*\".\" + 0.019*\"'\" + 0.019*\")\" + 0.016*\"(\"\n",
      "Topic #2:\n",
      "0.069*\".\" + 0.058*\",\" + 0.034*\"'\" + 0.019*\"``\" + 0.012*\"wa\"\n",
      "Topic #3:\n",
      "0.143*\"--\" + 0.040*\",\" + 0.029*\":\" + 0.028*\"1\" + 0.027*\"0\"\n",
      "Topic #4:\n",
      "0.132*\"#\" + 0.122*\";\" + 0.120*\"&\" + 0.111*\"9\" + 0.020*\")\"\n",
      "Topic #5:\n",
      "0.055*\"--\" + 0.049*\",\" + 0.036*\".\" + 0.023*\")\" + 0.023*\"(\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaModel(corpus=bow_corpus,id2word=dictionary,\n",
    "                                  chunksize=1740,alpha='auto',eta='auto',\n",
    "                                  random_state=42,iterations=500,\n",
    "                                  num_topics=5,passes=20,eval_every=None)\n",
    "for topic_id,topic in lda_model.print_topics(num_topics=10,num_words=5):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 10px\">\n",
    "观察发现，主题以标点居多，预处理应该去掉标点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.050*\"9\" + 0.008*\"use\" + 0.008*\"imag\" + 0.008*\"thi\" + 0.007*\"X\" + 0.006*\"25\" + 0.005*\"window\" + 0.005*\"program\" + 0.005*\"file\" + 0.004*\"avail\"\n",
      "Topic #2:\n",
      "0.014*\"thi\" + 0.013*\"wa\" + 0.011*\"would\" + 0.008*\"one\" + 0.006*\"ha\" + 0.006*\"car\" + 0.006*\"like\" + 0.006*\"know\" + 0.006*\"ani\" + 0.006*\"peopl\"\n",
      "Topic #3:\n",
      "0.012*\"thi\" + 0.009*\"use\" + 0.007*\"like\" + 0.006*\"gun\" + 0.004*\"ani\" + 0.004*\"one\" + 0.004*\"get\" + 0.004*\"window\" + 0.004*\"file\" + 0.003*\"onli\"\n",
      "Topic #4:\n",
      "0.037*\"1\" + 0.033*\"0\" + 0.017*\"2\" + 0.011*\"3\" + 0.009*\"4\" + 0.009*\"wa\" + 0.009*\"9\" + 0.007*\"5\" + 0.006*\"period\" + 0.005*\"6\"\n",
      "Topic #5:\n",
      "0.053*\"9\" + 0.020*\"2\" + 0.011*\"3\" + 0.010*\"7\" + 0.010*\"4\" + 0.007*\"1\" + 0.007*\"hockey\" + 0.006*\"5\" + 0.006*\"game\" + 0.006*\"team\"\n"
     ]
    }
   ],
   "source": [
    "#预处理去除标点\n",
    "norm_corpus=[]\n",
    "clean_article=multi5['Clean Article'].tolist()\n",
    "for i in clean_article:\n",
    "    remove = str.maketrans('','',string.punctuation) \n",
    "    i = i.translate(remove)\n",
    "    a=word_tokenize(i)\n",
    "    norm_corpus.append(a)\n",
    "\n",
    "labels=multi5['Target Label'].tolist()\n",
    "\n",
    "dictionary=gensim.corpora.Dictionary(norm_corpus)\n",
    "bow_corpus=[dictionary.doc2bow(text) for text in norm_corpus]\n",
    "\n",
    "lda_model = gensim.models.LdaModel(corpus=bow_corpus,id2word=dictionary,\n",
    "                                  chunksize=1740,alpha='auto',eta='auto',\n",
    "                                  random_state=42,iterations=500,\n",
    "                                  num_topics=5,passes=20,eval_every=None)\n",
    "for topic_id,topic in lda_model.print_topics(num_topics=10,num_words=10):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  计算主题的 coherence 分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5612024294162415"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence=gensim.models.CoherenceModel(model=lda_model,\n",
    "                                       corpus=bow_corpus,\n",
    "                                       texts=norm_corpus,\n",
    "                                      dictionary=dictionary,\n",
    "                                      coherence='c_v')\n",
    "avg_coherence=coherence.get_coherence()\n",
    "avg_coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 显示每个文档关联到最相关的主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    " \n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0: \n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    \n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Contribution %</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7950</td>\n",
       "      <td>thi, wa, would, one, ha, car, like, know, ani,...</td>\n",
       "      <td>Does anyone know of a VL-Bus video card based ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8429</td>\n",
       "      <td>9, use, imag, thi, X, 25, window, program, fil...</td>\n",
       "      <td>If you are a user of Autodesk 3D Concepts, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7792</td>\n",
       "      <td>thi, wa, would, one, ha, car, like, know, ani,...</td>\n",
       "      <td>\\nI'm not sure if this is free or shareware, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9628</td>\n",
       "      <td>thi, wa, would, one, ha, car, like, know, ani,...</td>\n",
       "      <td>what about\\n\\nqrttoppm &lt; file.dis | ppmtotga &gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>9, use, imag, thi, X, 25, window, program, fil...</td>\n",
       "      <td>================ -----------------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>thi, use, like, gun, ani, one, get, window, fi...</td>\n",
       "      <td>\\nHi,\\n  See Roger Grywalski's response to :\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5406</td>\n",
       "      <td>thi, wa, would, one, ha, car, like, know, ani,...</td>\n",
       "      <td>\\tI heard that there is a VESA driver for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>thi, use, like, gun, ani, one, get, window, fi...</td>\n",
       "      <td>: &gt;over where it places its temp files: it jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6159</td>\n",
       "      <td>9, use, imag, thi, X, 25, window, program, fil...</td>\n",
       "      <td>\\n\\nHi. Can anyone please give me some ftp sit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4827</td>\n",
       "      <td>9, use, imag, thi, X, 25, window, program, fil...</td>\n",
       "      <td>\\nI'm sure the whole newspaper is copyrighted....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Contribution %  \\\n",
       "0            0             1.0          0.7950   \n",
       "1            1             0.0          0.8429   \n",
       "2            2             1.0          0.7792   \n",
       "3            3             1.0          0.9628   \n",
       "4            4             0.0          0.9996   \n",
       "5            5             2.0          0.9799   \n",
       "6            6             1.0          0.5406   \n",
       "7            7             2.0          0.9952   \n",
       "8            8             0.0          0.6159   \n",
       "9            9             0.0          0.4827   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  thi, wa, would, one, ha, car, like, know, ani,...   \n",
       "1  9, use, imag, thi, X, 25, window, program, fil...   \n",
       "2  thi, wa, would, one, ha, car, like, know, ani,...   \n",
       "3  thi, wa, would, one, ha, car, like, know, ani,...   \n",
       "4  9, use, imag, thi, X, 25, window, program, fil...   \n",
       "5  thi, use, like, gun, ani, one, get, window, fi...   \n",
       "6  thi, wa, would, one, ha, car, like, know, ani,...   \n",
       "7  thi, use, like, gun, ani, one, get, window, fi...   \n",
       "8  9, use, imag, thi, X, 25, window, program, fil...   \n",
       "9  9, use, imag, thi, X, 25, window, program, fil...   \n",
       "\n",
       "                                               Paper  \n",
       "0  Does anyone know of a VL-Bus video card based ...  \n",
       "1  If you are a user of Autodesk 3D Concepts, and...  \n",
       "2  \\nI'm not sure if this is free or shareware, b...  \n",
       "3  what about\\n\\nqrttoppm < file.dis | ppmtotga >...  \n",
       "4  ================ -----------------------------...  \n",
       "5  \\nHi,\\n  See Roger Grywalski's response to :\\n...  \n",
       "6  \\tI heard that there is a VESA driver for the ...  \n",
       "7  : >over where it places its temp files: it jus...  \n",
       "8  \\n\\nHi. Can anyone please give me some ftp sit...  \n",
       "9  \\nI'm sure the whole newspaper is copyrighted....  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=article)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Contribution %', 'Keywords','Paper']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 查看所有主题在文档中的分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Num_Documents</th>\n",
       "      <th>% Total Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9, use, imag, thi, X, 25, window, program, fil...</td>\n",
       "      <td>476</td>\n",
       "      <td>0.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>thi, wa, would, one, ha, car, like, know, ani,...</td>\n",
       "      <td>217</td>\n",
       "      <td>0.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>thi, use, like, gun, ani, one, get, window, fi...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1, 0, 2, 3, 4, wa, 9, 5, period, 6</td>\n",
       "      <td>67</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9, 2, 3, 7, 4, 1, hockey, 5, game, team</td>\n",
       "      <td>70</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dominant_Topic                                     Topic_Keywords  \\\n",
       "1.0             0.0  9, use, imag, thi, X, 25, window, program, fil...   \n",
       "0.0             1.0  thi, wa, would, one, ha, car, like, know, ani,...   \n",
       "2.0             2.0  thi, use, like, gun, ani, one, get, window, fi...   \n",
       "4.0             3.0                 1, 0, 2, 3, 4, wa, 9, 5, period, 6   \n",
       "3.0             4.0            9, 2, 3, 7, 4, 1, hockey, 5, game, team   \n",
       "\n",
       "     Num_Documents  % Total Documents  \n",
       "1.0            476              0.476  \n",
       "0.0            217              0.217  \n",
       "2.0            170              0.170  \n",
       "4.0             67              0.067  \n",
       "3.0             70              0.070  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']].drop_duplicates().reset_index(drop=True)\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', '% Total Documents']\n",
    "df_dominant_topics.sort_values(by=\"Dominant_Topic\", ascending=True, inplace=True)\n",
    "df_dominant_topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. 用矩阵分解方法 LSI 重复以上实验，对比每个主题的 top-10 关键词以及coherence 分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_bow=gensim.models.LsiModel(bow_corpus,id2word=dictionary,\n",
    "                              num_topics=5,onepass=True,chunksize=1700\n",
    "                               ,power_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "-0.935*\"9\" + -0.204*\"25\" + -0.118*\"2\" + -0.090*\"1\" + -0.083*\"0\" + -0.063*\"3\" + -0.062*\"X\" + -0.059*\"4\" + -0.049*\"7\" + -0.038*\"thi\"\n",
      "Topic #2:\n",
      "0.498*\"2\" + 0.427*\"1\" + 0.425*\"0\" + 0.261*\"3\" + -0.226*\"9\" + 0.222*\"4\" + 0.188*\"7\" + 0.117*\"5\" + 0.106*\"hockey\" + 0.091*\"team\"\n",
      "Topic #3:\n",
      "-0.504*\"0\" + -0.503*\"1\" + 0.306*\"2\" + 0.202*\"7\" + 0.171*\"hockey\" + 0.150*\"team\" + 0.124*\"leagu\" + 0.119*\"nhl\" + 0.118*\"3\" + 0.115*\"new\"\n",
      "Topic #4:\n",
      "-0.377*\"X\" + -0.258*\"imag\" + -0.229*\"use\" + -0.214*\"thi\" + 0.200*\"25\" + -0.190*\"window\" + -0.148*\"program\" + -0.137*\"avail\" + -0.134*\"includ\" + 0.126*\"2\"\n",
      "Topic #5:\n",
      "-0.507*\"imag\" + 0.469*\"X\" + 0.197*\"window\" + -0.165*\"data\" + -0.145*\"tool\" + -0.136*\"process\" + 0.112*\"manag\" + -0.110*\"plot\" + 0.107*\"offer\" + -0.097*\"25\"\n"
     ]
    }
   ],
   "source": [
    "for topic_id,topic in lsi_bow.print_topics(num_topics=10,num_words=10):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算coherence值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44069500204321665"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence=gensim.models.CoherenceModel(model=lsi_bow,\n",
    "                                       corpus=bow_corpus,\n",
    "                                       texts=norm_corpus,\n",
    "                                      dictionary=dictionary,\n",
    "                                      coherence='c_v')\n",
    "avg_coherence=coherence.get_coherence()\n",
    "avg_coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比top-10关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>LSI</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Term1</td>\n",
       "      <td>9</td>\n",
       "      <td>thi</td>\n",
       "      <td>thi</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Term1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>X</td>\n",
       "      <td>imag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Term2</td>\n",
       "      <td>use</td>\n",
       "      <td>wa</td>\n",
       "      <td>use</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Term2</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>imag</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Term3</td>\n",
       "      <td>imag</td>\n",
       "      <td>would</td>\n",
       "      <td>like</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Term3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>use</td>\n",
       "      <td>window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Term4</td>\n",
       "      <td>thi</td>\n",
       "      <td>one</td>\n",
       "      <td>gun</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Term4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>thi</td>\n",
       "      <td>data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Term5</td>\n",
       "      <td>X</td>\n",
       "      <td>ha</td>\n",
       "      <td>ani</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Term5</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>hockey</td>\n",
       "      <td>25</td>\n",
       "      <td>tool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Term6</td>\n",
       "      <td>25</td>\n",
       "      <td>car</td>\n",
       "      <td>one</td>\n",
       "      <td>wa</td>\n",
       "      <td>1</td>\n",
       "      <td>Term6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>team</td>\n",
       "      <td>window</td>\n",
       "      <td>process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Term7</td>\n",
       "      <td>window</td>\n",
       "      <td>like</td>\n",
       "      <td>get</td>\n",
       "      <td>9</td>\n",
       "      <td>hockey</td>\n",
       "      <td>Term7</td>\n",
       "      <td>X</td>\n",
       "      <td>7</td>\n",
       "      <td>leagu</td>\n",
       "      <td>program</td>\n",
       "      <td>manag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Term8</td>\n",
       "      <td>program</td>\n",
       "      <td>know</td>\n",
       "      <td>window</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Term8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>nhl</td>\n",
       "      <td>avail</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Term9</td>\n",
       "      <td>file</td>\n",
       "      <td>ani</td>\n",
       "      <td>file</td>\n",
       "      <td>period</td>\n",
       "      <td>game</td>\n",
       "      <td>Term9</td>\n",
       "      <td>7</td>\n",
       "      <td>hockey</td>\n",
       "      <td>3</td>\n",
       "      <td>includ</td>\n",
       "      <td>offer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Term10</td>\n",
       "      <td>avail</td>\n",
       "      <td>peopl</td>\n",
       "      <td>onli</td>\n",
       "      <td>6</td>\n",
       "      <td>team</td>\n",
       "      <td>Term10</td>\n",
       "      <td>thi</td>\n",
       "      <td>team</td>\n",
       "      <td>new</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LDA   Topic1 Topic2  Topic3  Topic4  Topic5     LSI Topic1  Topic2  \\\n",
       "0   Term1        9    thi     thi       1       9   Term1      9       2   \n",
       "1   Term2      use     wa     use       0       2   Term2     25       1   \n",
       "2   Term3     imag  would    like       2       3   Term3      2       0   \n",
       "3   Term4      thi    one     gun       3       7   Term4      1       3   \n",
       "4   Term5        X     ha     ani       4       4   Term5      0       9   \n",
       "5   Term6       25    car     one      wa       1   Term6      3       4   \n",
       "6   Term7   window   like     get       9  hockey   Term7      X       7   \n",
       "7   Term8  program   know  window       5       5   Term8      4       5   \n",
       "8   Term9     file    ani    file  period    game   Term9      7  hockey   \n",
       "9  Term10    avail  peopl    onli       6    team  Term10    thi    team   \n",
       "\n",
       "   Topic3   Topic4   Topic5  \n",
       "0       0        X     imag  \n",
       "1       1     imag        X  \n",
       "2       2      use   window  \n",
       "3       7      thi     data  \n",
       "4  hockey       25     tool  \n",
       "5    team   window  process  \n",
       "6   leagu  program    manag  \n",
       "7     nhl    avail     plot  \n",
       "8       3   includ    offer  \n",
       "9     new        2       25  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "LSI_topic=lsi_bow.show_topics(log=True, formatted=False)\n",
    "LDA_topic=lda_model.show_topics(log=True, formatted=False)\n",
    "\n",
    "tmp=[]\n",
    "for i in range(10):\n",
    "    tmp.append('Term'+str(i+1))\n",
    "\n",
    "\n",
    "def get_list(topic):\n",
    "    aa=[]\n",
    "    for i in range(10):\n",
    "        bb=[]\n",
    "        for j in range(5):\n",
    "            bb.append(topic[j][1][i][0])\n",
    "        aa.append(bb)\n",
    "    return aa\n",
    "\n",
    "\n",
    "LSI_list=get_list(LSI_topic)\n",
    "LDA_list=get_list(LDA_topic)\n",
    "df_LSI = pd.DataFrame(LSI_list, columns=['Topic1', 'Topic2', 'Topic3','Topic4','Topic5'])\n",
    "df_LSI.insert(0,'LSI',tmp)\n",
    "\n",
    "df_LDA = pd.DataFrame(LDA_list, columns=['Topic1', 'Topic2', 'Topic3','Topic4','Topic5'])\n",
    "df_LDA.insert(0,'LDA',tmp)\n",
    "\n",
    "df_LSI=pd.concat([df_LDA,df_LSI],axis=1)\n",
    "df_LSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 10px\">\n",
    "对比coherence值可以发现，LDA的效果要更优，比LSI方式高出了12%。对比每个主题的关键词可以发现，虽然存在顺序不一致问题，但是还是能大致看出有些主题的关键词大致相似。如LDA的Topic1和LSI的Topic4，都有“imag”和“use”以及“program”等词，可以判断应该是有关程序设计的文章。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
